{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large language models\n",
    "\n",
    "![FigureP-1.Allpartsandchaptersofthebook]('./figures/FigureP-1.Allpartsandchaptersofthebook.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. An introduction to large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What Is Language AI / Natuaral Language Processing ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ A subfield of AI that focuses on developing technologies capable of understanding, processing, and generating human language\n",
    "\n",
    "\n",
    "![Figure1-2.LanguageAIiscapableofmanytasksbyprocessingtextualinput](./figures/Figure1-2.LanguageAIiscapableofmanytasksbyprocessingtextualinput.png)\n",
    "\n",
    "\n",
    "+ Bag-of-words: \n",
    "    - a method for representing unstructured text\n",
    "    - Tokenizing => Representation (model)\n",
    "    - Limitation: this approach ignores the semantic nature or meaning.  \n",
    "\n",
    "    ![Figure1-5.Abag-of-wordsiscreatedbycountingindividualwords.Thesevaluesarereferredtoasvectorrepresentations.](./figures/Figure1-5.Abag-of-wordsiscreatedbycountingindividualwords.Thesevaluesarereferredtoasvectorrepresentations.png)\n",
    "\n",
    "+ Word2vec (*neural networks*):\n",
    "    - capturing the meaning of text in *embeddings*\n",
    "    - nếu các từ có chung hàng xóm => thường sẽ có embedding giống nhau.\n",
    "    - embeddings can have many properties to represent the meaning of a word like: the size of embeddings is fixed.\n",
    "\n",
    "    ![Figure1-8.Thevaluesofembeddingsrepresentpropertiesthatareusedtorepresentwords](./figures/Figure1-8.Thevaluesofembeddingsrepresentpropertiesthatareusedtorepresentwords.png)\n",
    "\n",
    "\n",
    "+ Types of Embeddings:\n",
    "    - Word embedings\n",
    "    - Sentence embedings\n",
    "\n",
    "    ![Figure1-10.Embeddingscanbecreatedfordifferenttypesofinput](./figures/Figure1-10.Embeddingscanbecreatedfordifferenttypesofinput.png)\n",
    "\n",
    "+ Encoding and Decoding Context with Attention\n",
    "    - With w2v, same embedding regardless of context in which it's used. \n",
    "        - **NOTE**: tìm các ví dụ chứng minh\n",
    "    - RNN includes 2 tasks:\n",
    "        - encoding: representing an input sentence \n",
    "        - decoding: generating an output sentence\n",
    "\n",
    "    ![Figure1-11.Tworecurrentneuralnetworks(decoderandencoder)](./figures/Figure1-11.Tworecurrentneuralnetworks(decoderandencoder).png)\n",
    "\n",
    "    - Each step in this architecture is *autoregressive*\n",
    "        - When generating the next word, this architecture needs to consume all previously generated words.\n",
    "\n",
    "        ![Figure1-12.Eachpreviousoutputtokenisusedasinputtogeneratethenexttoken](./figures/Figure1-12.Eachpreviousoutputtokenisusedasinputtogeneratethenexttoken.png)\n",
    "\n",
    "+ Attention mechanism can be replace decoder\n",
    "    ![Figure1-14.Attentionallowsamodelto“attend”tocertainpartsofsequencesthatmightrelatemoreorlesstooneanother](./figures/Figure1-14.Attentionallowsamodelto“attend”tocertainpartsofsequencesthatmightrelatemoreorlesstooneanother.png)\n",
    "\n",
    "    - By adding these attention mechanism to the decoder step\n",
    "\n",
    "    ![Figure1-15.RNNwithAttention](./figures/Figure1-15.RNNwithAttention.png)\n",
    "\n",
    "+ Attention Is All You Need\n",
    "    - Based on *Transformer*:\n",
    "        - could be trained in parallel -> speed up training\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are large language models ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the common use cases and applications of large language models ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we use large language models ourselves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokens & Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lookig inside large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Using Pretrained Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Text clustering and topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Advanced Text Generation techniques and tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Semantic Search & Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Multimodel large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training & Fine-tuning Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Creating text embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Fine-tuning representation models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Fine-tuning Generation Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
