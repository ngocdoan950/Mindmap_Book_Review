{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large language models\n",
    "\n",
    "![FigureP-1.Allpartsandchaptersofthebook]('./figures/FigureP-1.Allpartsandchaptersofthebook.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. An introduction to large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What Is Language AI / Natuaral Language Processing ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ A subfield of AI that focuses on developing technologies capable of understanding, processing, and generating human language\n",
    "\n",
    "\n",
    "![Figure1-2.LanguageAIiscapableofmanytasksbyprocessingtextualinput](./figures/Figure1-2.LanguageAIiscapableofmanytasksbyprocessingtextualinput.png)\n",
    "\n",
    "\n",
    "+ Bag-of-words: \n",
    "    - a method for representing unstructured text\n",
    "    - Tokenizing => Representation (model)\n",
    "    - Limitation: this approach ignores the semantic nature or meaning.  \n",
    "\n",
    "    ![Figure1-5.Abag-of-wordsiscreatedbycountingindividualwords.Thesevaluesarereferredtoasvectorrepresentations.](./figures/Figure1-5.Abag-of-wordsiscreatedbycountingindividualwords.Thesevaluesarereferredtoasvectorrepresentations.png)\n",
    "\n",
    "+ Word2vec (*neural networks*):\n",
    "    - capturing the meaning of text in *embeddings*\n",
    "    - nếu các từ có chung hàng xóm => thường sẽ có embedding giống nhau.\n",
    "    - embeddings can have many properties to represent the meaning of a word like: the size of embeddings is fixed.\n",
    "\n",
    "    ![Figure1-8.Thevaluesofembeddingsrepresentpropertiesthatareusedtorepresentwords](./figures/Figure1-8.Thevaluesofembeddingsrepresentpropertiesthatareusedtorepresentwords.png)\n",
    "\n",
    "\n",
    "+ Types of Embeddings:\n",
    "    - Word embedings\n",
    "    - Sentence embedings\n",
    "\n",
    "    ![Figure1-10.Embeddingscanbecreatedfordifferenttypesofinput](./figures/Figure1-10.Embeddingscanbecreatedfordifferenttypesofinput.png)\n",
    "\n",
    "+ Encoding and Decoding Context with Attention\n",
    "    - With w2v, same embedding regardless of context in which it's used. \n",
    "        - **NOTE**: tìm các ví dụ chứng minh\n",
    "    - RNN includes 2 tasks:\n",
    "        - encoding: representing an input sentence \n",
    "        - decoding: generating an output sentence\n",
    "\n",
    "    ![Figure1-11.Tworecurrentneuralnetworks(decoderandencoder)](./figures/Figure1-11.Tworecurrentneuralnetworks(decoderandencoder).png)\n",
    "\n",
    "    - Each step in this architecture is *autoregressive*\n",
    "        - When generating the next word, this architecture needs to consume all previously generated words.\n",
    "\n",
    "        ![Figure1-12.Eachpreviousoutputtokenisusedasinputtogeneratethenexttoken](./figures/Figure1-12.Eachpreviousoutputtokenisusedasinputtogeneratethenexttoken.png)\n",
    "\n",
    "+ Attention mechanism can be replace decoder\n",
    "    \n",
    "    ![Figure1-14.Attentionallowsamodelto“attend”tocertainpartsofsequencesthatmightrelatemoreorlesstooneanother](./figures/Figure1-14.Attentionallowsamodelto“attend”tocertainpartsofsequencesthatmightrelatemoreorlesstooneanother.png)\n",
    "\n",
    "    - By adding these attention mechanism to the decoder step\n",
    "\n",
    "    ![Figure1-15.RNNwithAttention](./figures/Figure1-15.RNNwithAttention.png)\n",
    "\n",
    "+ Attention Is All You Need\n",
    "    - Based on *Transformer*:\n",
    "        - could be trained in parallel -> speed up training\n",
    "\n",
    "        ![Figure1-16.TheTransformerisacombinationofstackedencoderanddecoderblocks](./figures/Figure1-16.TheTransformerisacombinationofstackedencoderanddecoderblocks.png)\n",
    "\n",
    "    - \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are large language models ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the common use cases and applications of large language models ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we use large language models ourselves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokens & Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ What tokens are & the tokenization methods used to power LLMs\n",
    "\n",
    "    ![Figure2-1.Languagemodelsdealwithtextinsmallchunkscalledtokens](./figures/Figure2-1.Languagemodelsdealwithtextinsmallchunkscalledtokens.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_llm(model_name=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    device (str): cpu / cuda\n",
    "    \"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device,\n",
    "        torch_dtype='auto',\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    #\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/envs/rag_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7a8f2213314479a0939950d5e935d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/envs/rag_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Write an email applogizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
    "LL_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "#\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        LL_MODEL,\n",
    "        device_map='cuda',\n",
    "        torch_dtype='auto',\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "#\n",
    "tokenizer = AutoTokenizer.from_pretrained(LL_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/d/envs/rag_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:267\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(input_prompt, \n\u001b[1;32m      2\u001b[0m                       return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#  generate the text\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m generation_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/envs/rag_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/envs/rag_env/lib/python3.12/site-packages/transformers/generation/utils.py:1356\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;66;03m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;66;03m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;66;03m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;66;03m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m   1354\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m   1355\u001b[0m )\n\u001b[0;32m-> 1356\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;66;03m# 4. Define other model kwargs\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39moutput_attentions\n",
      "File \u001b[0;32m/mnt/d/envs/rag_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:269\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_prompt, \n",
    "                      return_tensors='pt')\n",
    "#  generate the text\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text embeddings (for sentences & whole docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embeddings beyond LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings for Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Lookig inside large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Using Pretrained Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Text clustering and topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Advanced Text Generation techniques and tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Semantic Search & Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Multimodel large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training & Fine-tuning Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Creating text embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Fine-tuning representation models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Fine-tuning Generation Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
